---
title: 'Understanding DB Optimizer via Sales Query Optimization'
date: '2025-02-04'
tags: ['markdown', 'db', 'features', 'optimizer']
draft: false
summary: Optimal Path Discovery
images: ['/static/images/data/optimizer.png']
description: 'Analysis of why relocating filters in subqueries did not result in significant performance gains.'
---

# Understanding DB Optimizer Behavior via Sales Query Optimization

This post documents the process of reviewing execution plans and testing improvement strategies to address performance degradation in sales history queries.

## 1. Existing Query Structure and Problem Identification

The original query used `UNION ALL` to combine positive sales and sales cancellations. The logical execution flow was as follows:

1.  **Aggregation**: Performing a `SUM` on the 'Surcharge' table (the largest dataset) based on order numbers and product sequences.
2.  **Join**: Joining the result with the 'Orders' table in the outer query.
3.  **Filtering**: Applying a filter to retain only orders from the companyâ€™s direct mall.

### Initial Hypothesis

Based on the logical structure, I assumed the database would aggregate the entire volume of surcharge data before joining and filtering. Consequently, I expected that pushing the filter into the subquery would reduce the volume of data to be aggregated, thereby significantly improving performance.

## 2. Test Results and Observations

Despite relocating the filter according to the hypothesis, **no significant performance difference was observed.** Despite the logical expectation that reducing the initial dataset would lower computational overhead, the results remained nearly identical. Further analysis revealed this was due to the optimization mechanisms of the **Database Optimizer**.

## 3. The Role of the DB Optimizer

A SQL statement only defines the objective (**What**), while the **DB Optimizer** determines the actual execution strategy (**How**).

- **Execution Plan Optimization**: The optimizer selects the lowest-cost path based on table statistics.
- **Predicate Push-down**: The filter I attempted to move manually was already being repositioned by the optimizer to the most efficient location (e.g., inside the subquery) during the query analysis phase.
- **Conclusion**: Regardless of how a user logically structures a SQL statement, the optimizer reconfigures it at the execution stage if it identifies a more efficient path that guarantees the same result.

## 4. Key Takeaways

- **Importance of Empirical Testing**: When tuning queries, empirical performance measurement in a real environment is more critical than making structural changes based on assumptions.
- **Analysis of Execution Plans**: I realized that analyzing the **Execution Plan (EXPLAIN)** generated internally by the DB is more fundamental to performance optimization than the SQL syntax itself.

---

# What I studied after the test

## 1. How Indexes Work: B-Tree Traversal

An index is like aÂ **'Table of Contents'**Â or anÂ **'Index'**Â in the back of a book. With an index, the database doesn't have to read the entire table; instead, it follows a B-Tree structure to quickly pinpoint the exact location (address) of the data.

- **Operation:**Â The search traverses from theÂ **Root Node**Â â†’Â **Branch Nodes**Â â†’Â **Leaf Nodes**Â to obtain the Record ID (RID). It then "jumps" directly to the actual table block to fetch the data.

## 2. Why Use a B-Tree for Indexing?

B-Trees are used because they provide consistent search performance (O(logN)) and maintain efficiency even with massive datasets.

- **Balanced:**Â The height of the tree remains consistent even as data is added or deleted, ensuring predictable performance.
- **Efficient Range Scans:**Â Unlike Hash indexes (which only support equality checks), B-Tree leaf nodes are linked together, making them exceptionally fast for scanning a range of data.

## 3. Index Conditions and SARGability

For an index to be effective, the columns used in theÂ `WHERE`Â clause orÂ `JOIN`Â conditions must match the index configuration.

- **Example:**Â If you have a composite index onÂ `(Name, Age)`Â but search only byÂ `Age`, the database might not be able to use the index effectively. This is due to theÂ **Left-Prefix Rule**, where the leftmost column must be present in the query to trigger the index.

## 4. The Importance of Statistics

TheÂ **Optimizer**Â relies on statistics to calculate theÂ **"Estimated Cost"**Â of a query.

- **The Risk:**Â If a table has 100 million rows but the statistics incorrectly report only 100, the Optimizer may mistakenly choose a slowÂ **Sequential Scan**Â (Full Table Scan) instead of an Index Scan.
- **Key Metrics:**Â Statistics track table size, column value distribution (histograms), and NULL ratios to provide the foundation for an optimal execution plan.

---

### ðŸ’¡ Pro Tip: Handling Bulk Inserts

Updating an index for every single row during a multi-million row insertion is highly inefficient. In production environments, engineers often use this strategy:

1. **Drop or Disable**Â the indexes.
2. **Perform Bulk Insertion**Â (e.g., using theÂ `COPY`Â command).
3. **Rebuild/Re-create**Â the indexes: Sorting the data once to build the index from scratch is significantly faster than updating it incrementally.

## Database Performance: It's All About the Optimizer and Scan Depth

Database performance is less about specific keywords and more aboutÂ **how the Optimizer utilizes indexes**Â andÂ **how much data it actually scans**.

### 1. TheÂ `OR`Â Condition

`OR`Â is one of the most challenging conditions for performance optimization.

- **Performance Impact:**Â If even one of the columns connected byÂ `OR`Â lacks an index, the Optimizer is likely to resort to aÂ **Full Table Scan**. Even if both columns are indexed, the process of combining them (**Index Merge**) incurs additional overhead.
- **The Solution:**Â UsingÂ `UNION ALL`Â is often more efficient. It allows each sub-query to independently and reliably utilize its respective index.

---

### 2.Â `EXISTS`Â vs.Â `NOT EXISTS`

These conditions operate asÂ **Semi-joins**Â and generally offer superior performance.

- **EXISTS:**Â It usesÂ **Short-circuit**Â logicâ€”the scan stops immediately once a matching record is found. This makes it incredibly fast for existence checks.
- **NOT EXISTS:**Â While it may be slightly slower thanÂ `EXISTS`Â (as it must verify the absence of data), it is significantly safer and faster thanÂ `NOT IN`.
- **Efficiency:**Â These are most effective when the main query returns many rows but the subquery results are relatively small.

---

### 3.Â `IN`Â vs.Â `NOT IN`

WhileÂ `IN`Â offers great readability, its performance varies based on data volume and distribution.

- **IN:**Â Internally, it is processed as a set ofÂ `OR`Â conditions or converted into aÂ `JOIN`. In modern databases, the performance gap betweenÂ `IN`Â andÂ `EXISTS`Â is minimal due to advanced optimization.
- **NOT IN (Caution):**Â This is often the most dangerous operator for performance. It frequently fails to utilize indexes.
- **The NULL Trap:**Â If the subquery result contains aÂ **NULL**, aÂ `NOT IN`Â clause may return no results or behave unpredictably. For large datasets, it is highly recommended to useÂ **`NOT EXISTS`**Â or aÂ **`LEFT JOIN + IS NULL`**Â pattern instead.
